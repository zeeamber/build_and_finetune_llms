{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8565c6",
   "metadata": {},
   "source": [
    "## Coding attention mechanisms\n",
    "Types of attention mechanisms -\n",
    "1. Simplified self-attention\n",
    "2. Self-attention\n",
    "3. Causal attention\n",
    "4. Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c050803",
   "metadata": {},
   "source": [
    "## Creating a simple self-attention (Without trainable weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930c73f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Embedded input sequence\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # Your (x1)\n",
    "        [0.55, 0.87, 0.66], # journey (x2)\n",
    "        [0.57, 0.85, 0.64], # starts (x3)\n",
    "        [0.22, 0.58, 0.33], # with (x4)\n",
    "        [0.77, 0.25, 0.10], # one (x5)\n",
    "        [0.05, 0.80, 0.55] # step (x6)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64788053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating attention scores\n",
    "query = inputs[1] # x2\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1803e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1865468a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d700d01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating attention score between query and input 0: tensor([0.4300, 0.1500, 0.8900])\n",
      "Calculating attention score between query and input 1: tensor([0.5500, 0.8700, 0.6600])\n",
      "Calculating attention score between query and input 2: tensor([0.5700, 0.8500, 0.6400])\n",
      "Calculating attention score between query and input 3: tensor([0.2200, 0.5800, 0.3300])\n",
      "Calculating attention score between query and input 4: tensor([0.7700, 0.2500, 0.1000])\n",
      "Calculating attention score between query and input 5: tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "for i, x_i in enumerate(inputs):\n",
    "    print(f\"Calculating attention score between query and input {i}: {x_i}\")\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471ff2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "# Calculating attention weights by normalizing attention scores\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(f\"Attention weights: {attn_weights_2_tmp}\")\n",
    "print(f\"Sum: {attn_weights_2_tmp.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec84403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2_tmp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35af77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing attention scores using softmax\n",
    "\n",
    "def softmax_naive(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    return exp_x / exp_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc504e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights (softmax): tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(f\"Attention weights (softmax): {attn_weights_2_naive}\")\n",
    "print(f\"Sum: {attn_weights_2_naive.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce72902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights (Pytorch softmax): tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Pytorch softmax implementation\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(f\"Attention weights (Pytorch softmax): {attn_weights_2}\")\n",
    "print(f\"Sum: {attn_weights_2.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c391094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence 0: tensor([0.4300, 0.1500, 0.8900])\n",
      "Attention weight for input 0: 0.13854756951332092\n",
      "Context vector so far: tensor([0.0596, 0.0208, 0.1233])\n",
      "Input sequence 1: tensor([0.5500, 0.8700, 0.6600])\n",
      "Attention weight for input 1: 0.2378913015127182\n",
      "Context vector so far: tensor([0.1904, 0.2277, 0.2803])\n",
      "Input sequence 2: tensor([0.5700, 0.8500, 0.6400])\n",
      "Attention weight for input 2: 0.23327402770519257\n",
      "Context vector so far: tensor([0.3234, 0.4260, 0.4296])\n",
      "Input sequence 3: tensor([0.2200, 0.5800, 0.3300])\n",
      "Attention weight for input 3: 0.12399158626794815\n",
      "Context vector so far: tensor([0.3507, 0.4979, 0.4705])\n",
      "Input sequence 4: tensor([0.7700, 0.2500, 0.1000])\n",
      "Attention weight for input 4: 0.10818186402320862\n",
      "Context vector so far: tensor([0.4340, 0.5250, 0.4813])\n",
      "Input sequence 5: tensor([0.0500, 0.8000, 0.5500])\n",
      "Attention weight for input 5: 0.15811361372470856\n",
      "Context vector so far: tensor([0.4419, 0.6515, 0.5683])\n",
      "Context vector(x2): tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Calculating the context vector by multiplying the embedded inputs with the corresponding attention weights\n",
    "\n",
    "query = inputs[1] # x2\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(f\"Input sequence {i}: {x_i}\")\n",
    "    print(f\"Attention weight for input {i}: {attn_weights_2[i]}\")\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "    print(f\"Context vector so far: {context_vec_2}\")\n",
    "print(f\"Context vector(x2): {context_vec_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a11c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing attention weights for all inputs.\n",
    "attn_scores = torch.empty((inputs.shape[0], inputs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9fc8b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "836ce409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing dot product of tensor([0.4300, 0.1500, 0.8900]) and tensor([0.4300, 0.1500, 0.8900])\n",
      "Computing dot product of tensor([0.4300, 0.1500, 0.8900]) and tensor([0.5500, 0.8700, 0.6600])\n",
      "Computing dot product of tensor([0.4300, 0.1500, 0.8900]) and tensor([0.5700, 0.8500, 0.6400])\n",
      "Computing dot product of tensor([0.4300, 0.1500, 0.8900]) and tensor([0.2200, 0.5800, 0.3300])\n",
      "Computing dot product of tensor([0.4300, 0.1500, 0.8900]) and tensor([0.7700, 0.2500, 0.1000])\n",
      "Computing dot product of tensor([0.4300, 0.1500, 0.8900]) and tensor([0.0500, 0.8000, 0.5500])\n",
      "Computing dot product of tensor([0.5500, 0.8700, 0.6600]) and tensor([0.4300, 0.1500, 0.8900])\n",
      "Computing dot product of tensor([0.5500, 0.8700, 0.6600]) and tensor([0.5500, 0.8700, 0.6600])\n",
      "Computing dot product of tensor([0.5500, 0.8700, 0.6600]) and tensor([0.5700, 0.8500, 0.6400])\n",
      "Computing dot product of tensor([0.5500, 0.8700, 0.6600]) and tensor([0.2200, 0.5800, 0.3300])\n",
      "Computing dot product of tensor([0.5500, 0.8700, 0.6600]) and tensor([0.7700, 0.2500, 0.1000])\n",
      "Computing dot product of tensor([0.5500, 0.8700, 0.6600]) and tensor([0.0500, 0.8000, 0.5500])\n",
      "Computing dot product of tensor([0.5700, 0.8500, 0.6400]) and tensor([0.4300, 0.1500, 0.8900])\n",
      "Computing dot product of tensor([0.5700, 0.8500, 0.6400]) and tensor([0.5500, 0.8700, 0.6600])\n",
      "Computing dot product of tensor([0.5700, 0.8500, 0.6400]) and tensor([0.5700, 0.8500, 0.6400])\n",
      "Computing dot product of tensor([0.5700, 0.8500, 0.6400]) and tensor([0.2200, 0.5800, 0.3300])\n",
      "Computing dot product of tensor([0.5700, 0.8500, 0.6400]) and tensor([0.7700, 0.2500, 0.1000])\n",
      "Computing dot product of tensor([0.5700, 0.8500, 0.6400]) and tensor([0.0500, 0.8000, 0.5500])\n",
      "Computing dot product of tensor([0.2200, 0.5800, 0.3300]) and tensor([0.4300, 0.1500, 0.8900])\n",
      "Computing dot product of tensor([0.2200, 0.5800, 0.3300]) and tensor([0.5500, 0.8700, 0.6600])\n",
      "Computing dot product of tensor([0.2200, 0.5800, 0.3300]) and tensor([0.5700, 0.8500, 0.6400])\n",
      "Computing dot product of tensor([0.2200, 0.5800, 0.3300]) and tensor([0.2200, 0.5800, 0.3300])\n",
      "Computing dot product of tensor([0.2200, 0.5800, 0.3300]) and tensor([0.7700, 0.2500, 0.1000])\n",
      "Computing dot product of tensor([0.2200, 0.5800, 0.3300]) and tensor([0.0500, 0.8000, 0.5500])\n",
      "Computing dot product of tensor([0.7700, 0.2500, 0.1000]) and tensor([0.4300, 0.1500, 0.8900])\n",
      "Computing dot product of tensor([0.7700, 0.2500, 0.1000]) and tensor([0.5500, 0.8700, 0.6600])\n",
      "Computing dot product of tensor([0.7700, 0.2500, 0.1000]) and tensor([0.5700, 0.8500, 0.6400])\n",
      "Computing dot product of tensor([0.7700, 0.2500, 0.1000]) and tensor([0.2200, 0.5800, 0.3300])\n",
      "Computing dot product of tensor([0.7700, 0.2500, 0.1000]) and tensor([0.7700, 0.2500, 0.1000])\n",
      "Computing dot product of tensor([0.7700, 0.2500, 0.1000]) and tensor([0.0500, 0.8000, 0.5500])\n",
      "Computing dot product of tensor([0.0500, 0.8000, 0.5500]) and tensor([0.4300, 0.1500, 0.8900])\n",
      "Computing dot product of tensor([0.0500, 0.8000, 0.5500]) and tensor([0.5500, 0.8700, 0.6600])\n",
      "Computing dot product of tensor([0.0500, 0.8000, 0.5500]) and tensor([0.5700, 0.8500, 0.6400])\n",
      "Computing dot product of tensor([0.0500, 0.8000, 0.5500]) and tensor([0.2200, 0.5800, 0.3300])\n",
      "Computing dot product of tensor([0.0500, 0.8000, 0.5500]) and tensor([0.7700, 0.2500, 0.1000])\n",
      "Computing dot product of tensor([0.0500, 0.8000, 0.5500]) and tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    }
   ],
   "source": [
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        print(f\"Computing dot product of {x_i} and {x_j}\")\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a47096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43c77aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For loops are slow. So use matrix multiplication\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba8c1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "820940a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
       "        [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
       "        [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70e4e70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "814a34d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f74f6f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e0332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence 0: tensor([0.4300, 0.1500, 0.8900])\n",
      "Attention weights for input 0: tensor([0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452])\n",
      "Input sequence 1: tensor([0.5500, 0.8700, 0.6600])\n",
      "Attention weights for input 1: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Multiplying 0.13854756951332092 with tensor([0.5500, 0.8700, 0.6600])\n",
      "Conext vector so far: tensor([0.0596, 0.0208, 0.1233])\n",
      "Multiplying 0.2378913015127182 with tensor([0.5500, 0.8700, 0.6600])\n",
      "Conext vector so far: tensor([0.1904, 0.2277, 0.2803])\n",
      "Multiplying 0.23327402770519257 with tensor([0.5500, 0.8700, 0.6600])\n",
      "Conext vector so far: tensor([0.3234, 0.4260, 0.4296])\n",
      "Multiplying 0.12399158626794815 with tensor([0.5500, 0.8700, 0.6600])\n",
      "Conext vector so far: tensor([0.3507, 0.4979, 0.4705])\n",
      "Multiplying 0.10818186402320862 with tensor([0.5500, 0.8700, 0.6600])\n",
      "Conext vector so far: tensor([0.4340, 0.5250, 0.4813])\n",
      "Multiplying 0.15811361372470856 with tensor([0.5500, 0.8700, 0.6600])\n",
      "Conext vector so far: tensor([0.4419, 0.6515, 0.5683])\n",
      "Input sequence 2: tensor([0.5700, 0.8500, 0.6400])\n",
      "Attention weights for input 2: tensor([0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565])\n",
      "Input sequence 3: tensor([0.2200, 0.5800, 0.3300])\n",
      "Attention weights for input 3: tensor([0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720])\n",
      "Input sequence 4: tensor([0.7700, 0.2500, 0.1000])\n",
      "Attention weights for input 4: tensor([0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295])\n",
      "Input sequence 5: tensor([0.0500, 0.8000, 0.5500])\n",
      "Attention weights for input 5: tensor([0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896])\n",
      "Context vector: tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Computing the context vector\n",
    "context_vec = torch.zeros(inputs.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_weight = attn_weights[i]\n",
    "    input_context_vector = context_vec[i]\n",
    "    print(f\"Input sequence {i}: {x_i}\")\n",
    "    print(f\"Attention weights for input {i}: {attn_weight}\")\n",
    "    # Multiply each input vector by its corresponding attention weight\n",
    "    # and accumulate the result in the context vector\n",
    "    for j, x_j in enumerate(attn_weight):\n",
    "        context_vec[i] += x_j * inputs[j]\n",
    "        if i == 1:\n",
    "            print(f\"Multiplying {x_j} with {x_i}\")\n",
    "            print(f\"Conext vector so far: {context_vec[i]}\")\n",
    "print(f\"Context vector: {context_vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c6368ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57e0d0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, for loops are slow. So use matrix multiplication\n",
    "context_vec = attn_weights @ inputs\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01feb6a",
   "metadata": {},
   "source": [
    "## Implementing self-attention with trainable weights\n",
    "This is also called <i>Scaled dot-product attention</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "343b775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # The second input element\n",
    "d_in = inputs.shape[1] # Input embedding size, d= 3\n",
    "d_out = 2 # Output embedding size, d_out = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d009794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f834f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weight matrices Wq, Wk, Wv\n",
    "\n",
    "torch.manual_seed(123)  # For reproducibility\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa496b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2961, 0.5166],\n",
       "        [0.2517, 0.6886],\n",
       "        [0.0740, 0.8665]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32f90a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute query, key, and value vectors for the second input element\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a863a605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25416d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute keys and values for all inputs\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab5dd9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3669, 0.7646],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.4361, 1.1156],\n",
       "        [0.2408, 0.6706],\n",
       "        [0.1827, 0.3292],\n",
       "        [0.3275, 0.9642]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "002b282d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1855, 0.8812],\n",
       "        [0.3951, 1.0037],\n",
       "        [0.3879, 0.9831],\n",
       "        [0.2393, 0.5493],\n",
       "        [0.1492, 0.3346],\n",
       "        [0.3221, 0.7863]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a07e9b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8524)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing attention scores using the query vector and keys\n",
    "\n",
    "key_2 = keys[1]  # Using the key for the second input element\n",
    "attn_score_22 = query_2.dot(key_2)\n",
    "attn_score_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64416c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all attention scores for the second input element\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c5cb96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing attention weights from attention scores by scaling (dividing by square root of embedding dimension of keys) the scores and using softmax\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62c429cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing context vector for the second input element. This is the weighted sum over the value vectors\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e87ce4",
   "metadata": {},
   "source": [
    "### Implementing a compact class for self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "496a3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attention_scores = queries @ keys.T # Omega\n",
    "        d_k = keys.shape[-1]\n",
    "        attention_weights = torch.softmax(attention_scores / d_k**0.5, dim=-1)\n",
    "        context_vector = attention_weights @ values\n",
    "        return context_vector\n",
    "    \n",
    "    def assign_custom_weights(self, W_query, W_key, W_value):\n",
    "        self.W_query = nn.Parameter(W_query)\n",
    "        self.W_key = nn.Parameter(W_key)\n",
    "        self.W_value = nn.Parameter(W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb167394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)  # For reproducibility\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "context_vec = sa_v1(inputs)\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ab7fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Self Attention class using PyTorch's Linear layers\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T  # Omega\n",
    "        d_k = keys.shape[-1]\n",
    "        attention_weights = torch.softmax(attention_scores / d_k**0.5, dim=-1)\n",
    "        context_vector = attention_weights @ values\n",
    "        return context_vector\n",
    "    \n",
    "    def retrieve_weights(self):\n",
    "        return self.W_query.weight, self.W_key.weight, self.W_value.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67548834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)  # For reproducibility\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "context_vec = sa_v2(inputs)\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a257e504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query weights: Parameter containing:\n",
      "tensor([[ 0.3161,  0.4568,  0.5118],\n",
      "        [-0.1683, -0.3379, -0.0918]], requires_grad=True)\n",
      "Key weights: Parameter containing:\n",
      "tensor([[ 0.4058, -0.4704,  0.2368],\n",
      "        [ 0.2134, -0.2601, -0.5105]], requires_grad=True)\n",
      "Value weights: Parameter containing:\n",
      "tensor([[ 0.2526, -0.1415, -0.1962],\n",
      "        [ 0.5191, -0.0852, -0.2043]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "v2_weights = sa_v2.retrieve_weights()\n",
    "print(f\"Query weights: {v2_weights[0]}\")\n",
    "print(f\"Key weights: {v2_weights[1]}\")\n",
    "print(f\"Value weights: {v2_weights[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e58e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since nn.Linear stores weights in transposed form, we need to transpose them to make them compatible with V1\n",
    "v2_weights_trans = (v2_weights[0].T, v2_weights[1].T, v2_weights[2].T)\n",
    "\n",
    "# Transfer weights from V2 to V1\n",
    "sa_v1.assign_custom_weights(*v2_weights_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "823a36bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can use the V1 class with the weights from V2. Verify that the output is the same from V1 and V2\n",
    "torch.manual_seed(123)  # For reproducibility\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691c49a",
   "metadata": {},
   "source": [
    "## Hiding future words with causal attention (masked attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d344fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
       "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
       "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
       "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
       "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
       "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "691ee7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
       "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
       "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89b61c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mask using PyTorch's tril function\n",
    "context_length = attn_weights.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3c159f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply attn_weights with the mask to zero-out the values above the diagonal\n",
    "masked_simple = attn_weights * mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ba73264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-normlize the masked attention weights\n",
    "masked_simple = masked_simple / masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1315287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Improved masking based on mathematical property of softmax function\n",
    "# Creating a mask with 1s above the diagonal and then replacing these 1s with -inf\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1) # Note that previously we used tril (Not triu) with diagonal arg was 0, which is default value\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29d6347d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "584ee25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87ab9ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, if we apply softmax to the masked attention scores, it will zero-out the values above the diagonal because softmax will treat -inf as 0\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12800a47",
   "metadata": {},
   "source": [
    "### Masking additional attention weights with dropout (Useful for reducing overfitting while training LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2e0b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrating dropout\n",
    "torch.manual_seed(123)  # For reproducibility\n",
    "dropout = nn.Dropout(p=0.5)  # 50% dropout rate\n",
    "example_tensor = torch.ones(6, 6)\n",
    "example_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed2baf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2., 2.],\n",
       "        [0., 2., 0., 0., 0., 0.],\n",
       "        [0., 0., 2., 0., 2., 0.],\n",
       "        [2., 2., 0., 0., 0., 2.],\n",
       "        [2., 0., 0., 0., 0., 2.],\n",
       "        [0., 2., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(example_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e361afe",
   "metadata": {},
   "source": [
    "After applying the dropout, randomly 50% elements of the attention weight matrix becomes zero. To compensate this reduction, rest of the active elements are scaled up by a factor of `1 / 0.5 = 2`. This balancing is crucial to maintain the overall balance of attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phase. Note that we don't apply dropout during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b58c379b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.4921, 0.0000, 0.4638, 0.0000, 0.0000],\n",
       "        [0.0000, 0.3966, 0.3968, 0.3775, 0.3941, 0.0000],\n",
       "        [0.3869, 0.3327, 0.0000, 0.0000, 0.3331, 0.3058]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets apply dropout to the attention weights\n",
    "dropout(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f82f6",
   "metadata": {},
   "source": [
    "## Implementing a compact class for causal attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b907cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building intuition of batch inputs\n",
    "example_batch = torch.stack((inputs, inputs), dim=0)\n",
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06904d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04f94757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing causal attention class\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Transpose dimentions 1 and 2, keep the batch dimension intact\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vector = attn_weights @ values\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28a674e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = example_batch.shape[1]  # Number of tokens in the input sequence\n",
    "ca = CausalAttention(d_in, d_out, context_length, dropout=0.0)\n",
    "context_vecs = ca(example_batch)\n",
    "context_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9eb4cb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5526, -0.0981],\n",
       "         [-0.5299, -0.1081]],\n",
       "\n",
       "        [[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5526, -0.0981],\n",
       "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2799c04",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b294ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper class to implement multi-head attention which uses multiple instances of causal attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4323431b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53bf2cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bfd6d8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = example_batch.shape[1]  # Number of tokens in the input sequence\n",
    "d_in = example_batch.shape[-1]  # Input embedding size\n",
    "d_out = 2  # Output embedding size\n",
    "num_heads = 2  # Number of attention heads\n",
    "mha = MultiHeadAttention(d_in, d_out, num_heads, context_length, dropout=0.0)\n",
    "context_vecs = mha(example_batch)\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ddba67f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b5da7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5740,  0.2216],\n",
       "         [-0.7320,  0.0155],\n",
       "         [-0.7774, -0.0546],\n",
       "         [-0.6979, -0.0817],\n",
       "         [-0.6538, -0.0957],\n",
       "         [-0.6424, -0.1065]],\n",
       "\n",
       "        [[-0.5740,  0.2216],\n",
       "         [-0.7320,  0.0155],\n",
       "         [-0.7774, -0.0546],\n",
       "         [-0.6979, -0.0817],\n",
       "         [-0.6538, -0.0957],\n",
       "         [-0.6424, -0.1065]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returning two-dimensional context vectors\n",
    "torch.manual_seed(123)\n",
    "d_out = 1  # Output embedding size\n",
    "mha = MultiHeadAttention(d_in, d_out, num_heads, context_length, dropout=0.0)\n",
    "context_vecs = mha(example_batch)\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b9ae4",
   "metadata": {},
   "source": [
    "### Implementing multi-head attention with weight splits\n",
    "The idea is to split the input into multiple heads by reshaping the projected query, key and value tensors and then combine the results form thease heads after computing attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07361d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"Output embedding size must be divisible by number of heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduces the projection dimension to match the desired output dimension\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Reshape the keys, queries and values to split them into multiple heads\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose from (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute dot product for each head\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # Masks truncated to the number of tokens in the input sequence\n",
    "\n",
    "        # Use mask to fill attention scores with -inf\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # Compute attention weights\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Compute context vector for each head\n",
    "        context_vector = (attn_weights @ values).transpose(1, 2) # Transpose back to (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "        # Combine heads where d_out = num_heads * head_dim\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out) # Note: Instead of using contiguous().view(), you can use reshape() as well, but contiguous ensures that the memory layout is correct.\n",
    "\n",
    "        # Apply an optional linear projection to combine the heads\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "948f1ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3190, 0.4858],\n",
       "         [0.2943, 0.3897],\n",
       "         [0.2856, 0.3593],\n",
       "         [0.2693, 0.3873],\n",
       "         [0.2639, 0.3928],\n",
       "         [0.2575, 0.4028]],\n",
       "\n",
       "        [[0.3190, 0.4858],\n",
       "         [0.2943, 0.3897],\n",
       "         [0.2856, 0.3593],\n",
       "         [0.2693, 0.3873],\n",
       "         [0.2639, 0.3928],\n",
       "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = example_batch.shape\n",
    "d_out = 2  # Output embedding size\n",
    "num_heads = 2  # Number of attention heads\n",
    "mha = MultiHeadAttention(d_in, d_out, num_heads, context_length, dropout=0.0)\n",
    "context_vecs = mha(example_batch)\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8823d2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43276680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "26a4e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a MultiHeadAttention instance for smallest GPT-2 model (117M parameters)\n",
    "batch_size, context_length, d_in = [12, 1024, 768] # 12 attention heads, 1024 tokens, 768 embedding size.\n",
    "d_out = 768  # Output embedding size\n",
    "num_heads = 12  # Number of attention heads\n",
    "mha = MultiHeadAttention(d_in, d_out, num_heads, context_length, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4359a26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b04244",
   "metadata": {},
   "source": [
    "### An alternate Multi-Head Attention with combined weights\n",
    "The idea is to a single combined weight matrix, QKV, instead of individual weight matrices for Query, Key and Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d903137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionCombinedQKV(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"Output embedding size must be divisible by number of heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_qkv = nn.Linear(d_in, d_out * 3, bias=qkv_bias)  # Combined weight matrix for Q, K and V\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # (b, num_tokens, d_in) -> (b, num_tokens, d_out * 3)\n",
    "        qkv = self.W_qkv(x)\n",
    "\n",
    "        # (b, num_tokens, d_out * 3) -> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.view(b, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        # (b, num_tokens, 3, num_heads, head_dim) -> (3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # (3, b, num_heads, num_tokens, head_dim) -> 3 times (b, num_heads, num_tokens, head_dim)\n",
    "        queries, keys, values = qkv.unbind(dim=0)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)  # (b, num_heads, num_tokens, num_tokens)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # (b, num_heads, num_tokens, num_tokens) -> (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        context_vec = context_vec.transpose(1, 2)  # Transpose back to (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, d_in)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "71fc2626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 8\n",
    "context_len = 1024\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "embeddings = torch.randn((batch_size, context_len, embed_dim), device=device)\n",
    "\n",
    "mha_combined_qkv = MultiHeadAttentionCombinedQKV(embed_dim, embed_dim, num_heads, context_len, dropout=0.0)\n",
    "context_vecs_combined = mha_combined_qkv(embeddings)\n",
    "context_vecs_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31865155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build_llm_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
